# ============================================================
# Video-to-SOP Generator - Environment Configuration
# ============================================================
# Copy this file to .env and fill in your values
# ============================================================

# ============================================================
# AI MODE SELECTION
# ============================================================
# Options: "API" (cloud) or "LOCAL" (GPU)
# API = Uses Gemini + Groq (requires API keys, pay per use)
# LOCAL = Uses Ollama + faster-whisper (free, requires GPU)
AI_MODE=LOCAL

# ============================================================
# API MODE SETTINGS (used when AI_MODE=API)
# ============================================================

# Google Gemini API Key (for vision analysis)
# Get from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key_here

# Groq API Key (for Whisper audio transcription)
# Get from: https://console.groq.com/keys
GROQ_API_KEY=your_groq_api_key_here

# Optional: OpenAI API Key (if using GPT-4o instead of Gemini)
OPENAI_API_KEY=your_openai_api_key_here

# ============================================================
# LOCAL MODE SETTINGS (used when AI_MODE=LOCAL)
# ============================================================

# Ollama server address
OLLAMA_HOST=http://localhost:11434

# Ollama Vision model to use
# ‚öôÔ∏è  Auto-detection: Leave empty for automatic GPU-based selection
# üìä Manual configuration:
#   - RTX 6000 PRO Blackwell (96GB): llama3.2-vision:90b
#   - RTX 4090 (24GB):               llama3.2-vision:11b
#   - RTX 3090 (24GB):               llama3.2-vision:11b
#   - RTX 4070 Ti (12GB):            llama3.2-vision:11b
# Run 'python gpu_detector.py' to see your recommendation
OLLAMA_MODEL=

# Local Whisper model size
# ‚öôÔ∏è  Auto-detection: Leave empty for automatic GPU-based selection
# Options: tiny, base, small, medium, large-v3
# Recommended for RTX 6000/4090: large-v3 (best quality)
WHISPER_MODEL=

# Whisper compute type (for faster-whisper)
# Options: float16, int8, int8_float16
# float16 = best quality (recommended for RTX series)
# int8 = faster but lower quality
WHISPER_COMPUTE_TYPE=float16

# ============================================================
# FLASK WEB APP SETTINGS
# ============================================================

# Secret key for Flask sessions (generate with: python -c "import secrets; print(secrets.token_hex(32))")
SECRET_KEY=your_secret_key_here

# Flask environment
FLASK_ENV=development
FLASK_DEBUG=1
