# ğŸš€ Implementace LokÃ¡lnÃ­ho GPU MÃ³du (Ollama + faster-whisper)

**Datum zahÃ¡jenÃ­:** 2026-02-02  
**Hardware:** RTX 6000 Blackwell PRO (96GB VRAM)  
**Status:** âœ… KompletnÃ­ - PÅ™ipraveno k testovÃ¡nÃ­

---

## ğŸ“‹ Checklist Implementace

### FÃ¡ze 1: Konfigurace
- [x] 1.1 Aktualizovat `.env.example` s novÃ½mi promÄ›nnÃ½mi âœ…
- [x] 1.2 Aktualizovat `requirements.txt` s novÃ½mi zÃ¡vislostmi âœ…

### FÃ¡ze 2: Implementace Whisper (LokÃ¡lnÃ­ transkripce)
- [x] 2.1 VytvoÅ™it `local_whisper.py` pro faster-whisper âœ…
- [x] 2.2 Upravit `whisper_transcription.py` pro hybridnÃ­ reÅ¾im âœ…

### FÃ¡ze 3: Implementace VLM (Ollama Vision)
- [x] 3.1 VytvoÅ™it `local_vlm.py` pro Ollama komunikaci âœ…
- [x] 3.2 Upravit `sop_analyzer.py` pro hybridnÃ­ reÅ¾im âœ…

### FÃ¡ze 4: Integrace
- [x] 4.1 Upravit `main.py` pro automatickou detekci reÅ¾imu âœ…
- [x] 4.2 Upravit `webapp/app.py` pro webovÃ© rozhranÃ­ âœ…

### FÃ¡ze 5: TestovÃ¡nÃ­ (ÄŒEKÃ NA UÅ½IVATELE)
- [ ] 5.1 Nainstalovat Ollama a stÃ¡hnout model
- [ ] 5.2 Nainstalovat Python zÃ¡vislosti
- [ ] 5.3 Test lokÃ¡lnÃ­ho Whisper
- [ ] 5.4 Test Ollama VLM
- [ ] 5.5 End-to-end test celÃ©ho pipeline

### FÃ¡ze 6: Dokumentace
- [x] 6.1 Tento dokument - LOCAL_GPU_IMPLEMENTATION.md âœ…
- [ ] 6.2 Aktualizovat hlavnÃ­ README.md (po testovÃ¡nÃ­)

---

## ğŸ”§ CO MUSÃÅ  UDÄšLAT TY (prerekvizity)

### Krok 1: Nainstalovat Ollama
```powershell
# StÃ¡hni instalÃ¡tor z:
# https://ollama.com/download/windows

# Po instalaci ovÄ›Å™:
ollama --version
```

### Krok 2: StÃ¡hnout Vision model
```powershell
# Pro tvÃ½ch 96GB VRAM - nejlepÅ¡Ã­ kvalita (90B parametrÅ¯):
ollama pull llama3.2-vision:90b

# Alternativy (menÅ¡Ã­, rychlejÅ¡Ã­):
# ollama pull qwen2.5-vl:72b
# ollama pull llava:34b

# OvÄ›Å™enÃ­, Å¾e model je staÅ¾enÃ½:
ollama list
```

### Krok 3: Spustit Ollama server
```powershell
# Ollama musÃ­ bÄ›Å¾et na pozadÃ­:
ollama serve

# Nebo spusÅ¥ Ollama pÅ™es GUI (po instalaci bÄ›Å¾Ã­ automaticky)
```

### Krok 4: Nainstalovat Python zÃ¡vislosti
```powershell
# Aktivuj venv:
.\venv\Scripts\activate

# Nainstaluj novÃ© zÃ¡vislosti:
pip install -r requirements.txt
```

### Krok 5: VytvoÅ™it .env soubor
```powershell
# ZkopÃ­ruj Å¡ablonu:
copy .env.example .env

# Uprav .env soubor - nastav tyto hodnoty:
```

```ini
# HlavnÃ­ pÅ™epÃ­naÄ - LOCAL = GPU, API = Cloud
AI_MODE=LOCAL

# Ollama konfigurace
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2-vision:90b

# Whisper konfigurace
WHISPER_MODEL=large-v3
WHISPER_COMPUTE_TYPE=float16
```

### Krok 6: Otestovat
```powershell
# Test Ollama pÅ™ipojenÃ­:
python local_vlm.py

# Test celÃ©ho pipeline:
python main.py "cesta/k/testovaci.mp4" -o test_output.pdf
```

---

## ğŸ“Š Progress Log

| ÄŒas | Akce | Status |
|-----|------|--------|
| 12:49 | VytvoÅ™en tracking dokument | âœ… |
| 12:50 | AktualizovÃ¡n .env.example | âœ… |
| 12:50 | AktualizovÃ¡n requirements.txt | âœ… |
| 12:51 | VytvoÅ™en local_whisper.py | âœ… |
| 12:52 | VytvoÅ™en local_vlm.py | âœ… |
| 12:53 | Upraven whisper_transcription.py | âœ… |
| 12:54 | Upraven sop_analyzer.py | âœ… |
| 12:55 | Upraven main.py | âœ… |
| 12:56 | Upraven webapp/app.py | âœ… |
| **---** | **IMPLEMENTACE KOMPLETNÃ** | **âœ…** |

---

## ğŸ“ NovÃ©/UpravenÃ© soubory

| Soubor | Typ | Popis | Status |
|--------|-----|-------|--------|
| `local_whisper.py` | **NEW** | LokÃ¡lnÃ­ Whisper pÅ™es faster-whisper | âœ… |
| `local_vlm.py` | **NEW** | Ollama VLM klient | âœ… |
| `sop_analyzer.py` | MODIFIED | HybridnÃ­ reÅ¾im (API/Local) | âœ… |
| `whisper_transcription.py` | MODIFIED | HybridnÃ­ reÅ¾im (API/Local) | âœ… |
| `main.py` | MODIFIED | Detekce reÅ¾imu + prerekvizity | âœ… |
| `.env.example` | MODIFIED | NovÃ© promÄ›nnÃ© pro LOCAL mÃ³d | âœ… |
| `requirements.txt` | MODIFIED | faster-whisper, httpx | âœ… |
| `webapp/app.py` | MODIFIED | WebovÃ© rozhranÃ­ s hybridnÃ­m mÃ³dem | âœ… |

---

## ğŸ¯ CÃ­lovÃ¡ architektura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        main.py                               â”‚
â”‚              (automatickÃ¡ detekce AI_MODE)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AI_MODE = ? (z .env)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                              â”‚
           â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AI_MODE = "API"    â”‚      â”‚  AI_MODE = "LOCAL"   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Gemini API         â”‚      â”‚ â€¢ Ollama VLM         â”‚
â”‚ â€¢ Groq Whisper       â”‚      â”‚ â€¢ faster-whisper     â”‚
â”‚ â€¢ Cloud processing   â”‚      â”‚ â€¢ RTX 6000 GPU       â”‚
â”‚ â€¢ Pay per use        â”‚      â”‚ â€¢ Zero cost          â”‚
â”‚ â€¢ VyÅ¾aduje API klÃ­Äe â”‚      â”‚ â€¢ VyÅ¾aduje Ollama    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Jak spustit

### LokÃ¡lnÃ­ mÃ³d (GPU) - DOPORUÄŒENO PRO TEBE:
```powershell
# 1. Ujisti se, Å¾e Ollama bÄ›Å¾Ã­:
ollama serve

# 2. MÄ›j v .env nastaveno AI_MODE=LOCAL

# 3. CLI verze:
python main.py "cesta/k/videu.mp4" -o vystup.pdf

# 4. Nebo web verze:
cd webapp
python app.py
# OtevÅ™i http://localhost:5000
```

### API mÃ³d (Cloud):
```powershell
# 1. Nastav v .env:
#    AI_MODE=API
#    GOOGLE_API_KEY=tvuj_klic
#    GROQ_API_KEY=tvuj_klic

# 2. SpusÅ¥:
python main.py "cesta/k/videu.mp4" -o vystup.pdf
```

---

## âš™ï¸ KompletnÃ­ .env konfigurace

```ini
# ============================================================
# AI MODE SELECTION
# ============================================================
# Options: "API" (cloud) or "LOCAL" (GPU)
AI_MODE=LOCAL

# ============================================================
# LOCAL MODE SETTINGS (used when AI_MODE=LOCAL)
# ============================================================

# Ollama server address
OLLAMA_HOST=http://localhost:11434

# Ollama Vision model
# Pro 96GB VRAM: llama3.2-vision:90b (nejlepÅ¡Ã­)
# Alternativy: qwen2.5-vl:72b, llava:34b
OLLAMA_MODEL=llama3.2-vision:90b

# Local Whisper model size
# Options: tiny, base, small, medium, large-v3
WHISPER_MODEL=large-v3

# Whisper compute type
# float16 = best quality, int8 = faster
WHISPER_COMPUTE_TYPE=float16

# ============================================================
# API MODE SETTINGS (used when AI_MODE=API)
# ============================================================

# Google Gemini API Key
GOOGLE_API_KEY=your_key_here

# Groq API Key (for Whisper)
GROQ_API_KEY=your_key_here

# ============================================================
# FLASK WEB APP
# ============================================================
SECRET_KEY=your_secret_key_here
```

---

## ğŸ” Troubleshooting

### "Ollama not responding"
```powershell
# SpusÅ¥ Ollama server:
ollama serve

# Nebo zkontroluj, zda bÄ›Å¾Ã­:
curl http://localhost:11434/api/tags
```

### "Model not found"
```powershell
# StÃ¡hni model:
ollama pull llama3.2-vision:90b

# OvÄ›Å™ dostupnÃ© modely:
ollama list
```

### "CUDA not available"
```powershell
# OvÄ›Å™ CUDA instalaci:
python -c "import torch; print(torch.cuda.is_available(), torch.cuda.get_device_name(0))"

# Pokud vrÃ¡tÃ­ False, nainstaluj PyTorch s CUDA:
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

### "faster-whisper import error"
```powershell
pip install faster-whisper
```

### "httpx import error"
```powershell
pip install httpx
```

---

## ğŸ“ˆ OÄekÃ¡vanÃ½ vÃ½kon

| Operace | API Mode | LOCAL Mode (96GB VRAM) |
|---------|----------|------------------------|
| Whisper transkripce (4min video) | ~30s | **~5-8s** |
| VLM analÃ½za (20 snÃ­mkÅ¯) | ~75s | **~20-40s** |
| PDF generace | ~5s | ~5s |
| **Celkem** | ~2 min | **~30-60s** |
| **NÃ¡klady** | $0.01-0.05/video | **$0** |

---

## âœ… HOTOVO!

Implementace je kompletnÃ­. NynÃ­ proveÄ kroky v sekci **"CO MUSÃÅ  UDÄšLAT TY"** vÃ½Å¡e.

Po instalaci Ollama a staÅ¾enÃ­ modelu mÅ¯Å¾eÅ¡ otestovat pÅ™Ã­kazem:
```powershell
python main.py "tvoje_video.mp4" -o test.pdf
```

Pokud narazÃ­Å¡ na problÃ©my, podÃ­vej se do sekce Troubleshooting nebo se zeptej.
